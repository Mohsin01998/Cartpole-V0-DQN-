{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acdb076",
   "metadata": {},
   "source": [
    "# What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fed0b2",
   "metadata": {},
   "source": [
    "- It is the training of ML models to make a sequence of decisions\n",
    "- The method employs episodes and Trial versions to get solution\n",
    "- Rewards are given by games (it can be a positive and negative rewards). We focus on getting positive rewards.\n",
    "- Goal is to Maximize the Total Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f7e17",
   "metadata": {},
   "source": [
    "## What is Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e7c3d",
   "metadata": {},
   "source": [
    "##### Keras is one of the leading high-level neural networks APIs. It is written in Python and supports multiple back-end neural network computation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe68bda",
   "metadata": {},
   "source": [
    "- With Keras we do not need to make backpropogation algorithms(it is built in keras module)\n",
    "- Many Layers could be added in just few lines of code\n",
    "- All the types of models are built on same principles hence it becomes easier to master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ac1cb",
   "metadata": {},
   "source": [
    "## What is OpenAI Gym?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5633553",
   "metadata": {},
   "source": [
    "- Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "- It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "- Has easy implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab8bd5",
   "metadata": {},
   "source": [
    "## Learning On CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae8290",
   "metadata": {},
   "source": [
    "- A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart.\n",
    "- The pendulum starts upright, and the goal is to prevent it from falling over.\n",
    "- A reward of +1 is provided for every timestep that the pole remains upright.\n",
    "- The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ee124",
   "metadata": {},
   "source": [
    "## Understanding OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753a652",
   "metadata": {},
   "source": [
    "- OpenAI Gym environments are structured around two main parts: an observation space and an action space\n",
    "- Based On The current state of observation, we determine the action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabce532",
   "metadata": {},
   "source": [
    "## Using Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af859b2a",
   "metadata": {},
   "source": [
    "- create environment of gym env = gym.make('env-name')\n",
    "- reset the environment env.reset()\n",
    "- render the environment onto visible game env.render()\n",
    "- Take next step in game env.step('give your action')\n",
    "- close the rendering window env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c09207",
   "metadata": {},
   "source": [
    "### import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3cc7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #gym use for agent and environment(reinforcement learning algorithms)\n",
    "import numpy as np #numpy use for matrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e2f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# Create an environment of gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65dbf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "#reset the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb0542",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eadc2c",
   "metadata": {},
   "source": [
    "- We will collect data by running a certain number of Random trials\n",
    "- Only those trials will be considered that have got us a min score\n",
    "- One Hot Encoding will be used for passing action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6870722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take random data for 10,000 steps\n",
    "#The more data you run the more accurate results you gain\n",
    "#Neural netwrorks need maximum amount of data for scores or total rewards\n",
    "\n",
    "def collect_data(env):\n",
    "    num_episodes = 10000\n",
    "    # score is between 0 to 200 (0 for minimum and 200 for maximum)\n",
    "    \n",
    "    # I will take 50 for average because of random data \n",
    "    min_score = 50 \n",
    "    \n",
    "    #300 maximum steps at every episode/trial(usually it does not exceed 200 but for safety purpose we are taking 300 steps)\n",
    "    t_steps = 300\n",
    "    \n",
    "    #X is for input and y is for output in trainig data for modeling\n",
    "    training_X,training_y = [],[]\n",
    "    \n",
    "    #declare scores as empty as current score for comparison(array)\n",
    "    scores = []\n",
    "    \n",
    "    #Iteration of number of trials\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset() #reset environment after each game\n",
    "        score = 0  #declare score as integer\n",
    "        training_sample_X,training_sample_y = [],[] #declare training sample data for current  \n",
    "        \n",
    "    #show only 400th time game in order to save time\n",
    "        for step in range(t_steps):\n",
    "            if(episode%400==0):\n",
    "                env.render()\n",
    "                \n",
    "            #Take action randomly\n",
    "            action = np.random.randint(0,2) # left or right\n",
    "            \n",
    "            #Convert action in one hot as Keras also recommend it\n",
    "            #Keras also recommend it because it makes easier for modeling \n",
    "            one_hot_action = np.zeros(2) \n",
    "            \n",
    "            #randomly action equal to 1 in one hot \n",
    "            one_hot_action[action] = 1\n",
    "            \n",
    "            #store current training data by append\n",
    "            #4 input velocity,angel,angular velocity, position of poles store in observation\n",
    "            training_sample_X.append(obs)\n",
    "            training_sample_y.append(one_hot_action) #action is left(0) or right(1)\n",
    "            \n",
    "            #env.step() function returns observation,reward,done,info\n",
    "            #observation keeps input as I have mentioned above\n",
    "            #In reward 199 time step is maximum and 0 is minimum   \n",
    "            #maximum time steps are 190 after 190, game will be done. In that time you win or lose\n",
    "            \n",
    "            observation , reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            if done: \n",
    "                break \n",
    "            #If your game is finised or done then it must be stop otherwise it is not useful. \n",
    "                \n",
    "        #We will take scores only greater than minimum scores for training data               \n",
    "        if score>min_score:\n",
    "            scores.append(score)\n",
    "            \n",
    "            #store sample training variables into main training variables.\n",
    "            training_X+=training_sample_X \n",
    "            training_y+=training_sample_y\n",
    "    \n",
    "    #Convert training input and output in array form\n",
    "    training_X,training_y = np.array(training_X), np.array(training_y)\n",
    "    \n",
    "    #Pint mean and median of scores\n",
    "    print(\"Average: {}\".format(np.mean(scores)))\n",
    "    print(\"Median: {}\".format(np.median(scores)))\n",
    "    \n",
    "    return training_X,training_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffca425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 61.75757575757576\n",
      "Median: 59.0\n",
      "(array([[ 0.03986029, -0.04820817, -0.03865588, -0.04262823],\n",
      "       [ 0.03986029, -0.04820817, -0.03865588, -0.04262823],\n",
      "       [ 0.03986029, -0.04820817, -0.03865588, -0.04262823],\n",
      "       ...,\n",
      "       [-0.01718096, -0.00128567,  0.00411215, -0.04054484],\n",
      "       [-0.01718096, -0.00128567,  0.00411215, -0.04054484],\n",
      "       [-0.01718096, -0.00128567,  0.00411215, -0.04054484]],\n",
      "      dtype=float32), array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 1.],\n",
      "       ...,\n",
      "       [0., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(collect_data(env))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a9e4a",
   "metadata": {},
   "source": [
    "#### Average is 63.2 and median is 60 in minimum score 50. It is quite good average according to minimum score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e34ead",
   "metadata": {},
   "source": [
    "##### Training X is showing four elements (position,velocity,angular velocity,angel) and y is showing 0,1 (left,right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773ed88",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c01e1",
   "metadata": {},
   "source": [
    "- We will use keras for model definition\n",
    "- The model we use here is a very simple one: several fully-connected layers\n",
    "- We can use enhancement such as Convolutions, LSTM,Dropouts etc.\n",
    "- Input will be the observation and output will be action\n",
    "- Loss can be used are mean_squared_error, categorical_crossentropy etc.\n",
    "- Preferred optimizer is usually adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330a458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential #use for sequentional series of neural network\n",
    "from tensorflow.keras.layers import Dense,Dropout #Dense layers are most basic and important layers of deep Q network(hidden layers) \n",
    "#Dropout function is use to drop some % deep q network or neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1751f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Our_model():\n",
    "    model = Sequential() #basic model for deep Q network\n",
    "    model.add(Dense(128,input_shape=(4,),activation='relu')) # output is 128 layers, input size is 4\n",
    "    # relu function has its derivative both are monotonic.\n",
    "    model.add(Dropout(0.6)) #it removes 60% of neurons from the neural network to overcome the overfitting\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "     \n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    #Softmax function takes higher probability value amongst all\n",
    "    #activation function in a neural network model\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    #summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    #Taking mean square error for finding loss \n",
    "    #we will use adam for optimization algorithm\n",
    "    #accuracy will be measured by metrics\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017975f7",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a931890",
   "metadata": {},
   "source": [
    "- From the data collection and model above we train our data\n",
    "- We will go through several trials to check on multiple cases\n",
    "- In each trial we get a score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58beb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    env1 = gym.make('CartPole-v0') #create new environment\n",
    "    training_X,training_y = collect_data(env1) #gather the data from the above function\n",
    "    model = Our_model() # call created model function in variable model\n",
    "    model.fit(training_X,training_y,epochs=5) #fit training data into model\n",
    "    #In each epoch sending the data from the input layer to output layer\n",
    "    #We increase epoch value for better accuracy \n",
    "    \n",
    "    #create new environment for new games\n",
    "    scores = []\n",
    "    num_trails = 50 #for 50 trial games\n",
    "    t_steps = 300\n",
    "    \n",
    "    for trial in range(num_trails):\n",
    "        obs = env1.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for step in range(t_steps):\n",
    "            if(trial%4==0): #takes every 4th trail only to render\n",
    "                env1.render()\n",
    "            \n",
    "            #argmax function takes maximum value of arguement\n",
    "            #Use model for prediction\n",
    "            #observation will be change after every game\n",
    "            #model will predict on the base of observation\n",
    "            \n",
    "            action = np.argmax(model.predict(obs.reshape(1,4)))\n",
    "            observation,reward,done,info = env1.step(action)\n",
    "            \n",
    "            #if game is finished or done, the reward will be store in score\n",
    "            if done:\n",
    "                score+=reward\n",
    "                break\n",
    "                \n",
    "        #update scores        \n",
    "        scores.append(score)\n",
    "        \n",
    "        #print mean of the scores\n",
    "        print(np.mean(scores))\n",
    "        \n",
    "    #close the environment    \n",
    "    env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bea437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 62.67966573816156\n",
      "Median: 59.0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               640       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 329,730\n",
      "Trainable params: 329,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "704/704 [==============================] - 4s 5ms/step - loss: 0.2507 - accuracy: 0.4994\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 4s 5ms/step - loss: 0.2502 - accuracy: 0.5069\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 4s 5ms/step - loss: 0.2502 - accuracy: 0.4993\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 3s 5ms/step - loss: 0.2503 - accuracy: 0.4940\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 4s 6ms/step - loss: 0.2500 - accuracy: 0.5015\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#call the predict function\n",
    "predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb40b0",
   "metadata": {},
   "source": [
    "##### The results you can see is 1.0 in every game which shows the pole is consistantly on upright (perpendicular) neither it is exceeding their boundries. Due to the 60% of dropout function accuracy is 60% which is showing it is not overfitting model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e03b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
